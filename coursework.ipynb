{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Big Data in Finance II  Group Assignment"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d6e0f590b58e58b5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Question1.\tIn the data used by Gu, Kelly and Xiu (RFS 2019 – provided in class), use a similar procedure to theirs to predict stock returns with neural networks. Start by finding a suitable baseline configuration, and use a validation procedure to pick optimal hyperparameters for three neural network models: One with 2 hidden layers, one with 3 hidden layers, and one with 4 hidden layers."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ece920b5884e5aa3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Import the packages and data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2cab95a370bbbf"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-17 13:06:18.107377: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization\n",
    "import optuna\n",
    "\n",
    "# fix random state\n",
    "random_state = 42\n",
    "\n",
    "panel = pd.read_pickle('returns_chars_panel.pkl') \n",
    "macro = pd.read_pickle('macro_timeseries.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T12:06:25.715713Z",
     "start_time": "2024-05-17T12:06:10.743550Z"
    }
   },
   "id": "2346ab6b1bc19c07"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Process the data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "573471148b566ca8"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# combine micro and macro data\n",
    "df = pd.merge(panel,macro,on='date',how='left',suffixes=['','_macro']) \n",
    "\n",
    "# features + targets \n",
    "X = df.drop(columns=['ret','excess_ret','rfree','permno','date']) # everything except return info and IDs\n",
    "y = df['excess_ret'] \n",
    "\n",
    "# make 30 years of training data\n",
    "date = df['date']\n",
    "training = (date <= '2006-03') # selects \n",
    "X_train, y_train = X.loc[training].values, y.loc[training].values \n",
    "\n",
    "# make test data\n",
    "test = (date > '2006-03') \n",
    "X_test, y_test = X.loc[test].values, y.loc[test].values "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T13:18:39.083541Z",
     "start_time": "2024-05-17T13:17:15.079936Z"
    }
   },
   "id": "5e9409b866557ff"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "3.896265065762339"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train) / len(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T13:19:42.866270Z",
     "start_time": "2024-05-17T13:19:42.845272Z"
    }
   },
   "id": "fabb4960501a341"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train the NN with optuna"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab3a2819f033440d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-17 14:21:07,490] A new study created in memory with name: no-name-7818be5c-3c89-48b5-8e82-822402ce9dbe\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def create_model(trial, num_layers):\n",
    "    neurons_per_layer = trial.suggest_categorical('neurons_per_layer', [32, 64, 128, 256])\n",
    "    activation = trial.suggest_categorical('activation', ['relu', 'tanh'])\n",
    "    optimizer = trial.suggest_categorical('optimizer', ['adam', 'sgd'])\n",
    "    learning_rate = trial.suggest_categorical('learning_rate', [0.001, 0.0001])\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(X_train.shape[1],)))\n",
    "    model.add(Dense(neurons_per_layer, activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    for _ in range(num_layers - 1):\n",
    "        model.add(Dense(neurons_per_layer, activation=activation))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(1))  # Output layer for regression\n",
    "\n",
    "    if optimizer == 'adam':\n",
    "        opt = Adam(learning_rate=learning_rate)\n",
    "    else:\n",
    "        opt = SGD(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=opt, loss='mean_squared_error', metrics=['mae'])\n",
    "    return model, batch_size\n",
    "\n",
    "def objective(trial, num_layers):\n",
    "    model, batch_size = create_model(trial, num_layers)\n",
    "\n",
    "    # K-Fold Cross Validation\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "    val_scores = []\n",
    "\n",
    "    for train_index, val_index in kf.split(X_train):\n",
    "        X_tr, X_val = X_train[train_index], X_train[val_index]\n",
    "        y_tr, y_val = y_train[train_index], y_train[val_index]\n",
    "\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=5)\n",
    "        model.fit(X_tr, y_tr, epochs=2, batch_size=batch_size, validation_data=(X_val, y_val), callbacks=[es], verbose=0)\n",
    "\n",
    "        val_loss, val_mae = model.evaluate(X_val, y_val, verbose=0)\n",
    "        val_scores.append(val_mae)\n",
    "\n",
    "    return np.mean(val_scores)\n",
    "\n",
    "# Create a study for each number of layers and optimize\n",
    "studies = {}\n",
    "num_layers_options = [2, 3, 4]\n",
    "\n",
    "for num_layers in num_layers_options:\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(lambda trial: objective(trial, num_layers), n_trials=5)\n",
    "    studies[num_layers] = study\n",
    "    print(f'Best hyperparameters for {num_layers} layers: {study.best_params}')\n",
    "    print(f'Validation MAE for {num_layers} layers: {study.best_value}')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-05-17T13:21:07.496099Z"
    }
   },
   "id": "4b25690bd64be2e6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Write the analysis here."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "df5d04beee3f0883"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Question2.\tUse test data to get an idea of the out of sample performance of each model. Convert the standard MSE metric for out of sample performance to the “R2 out of sample” metric that was discussed in class. Compare your results to those in Gu-Kelly-Xiu and comment on the differences. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "23661be82e6d6efd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define a function to calculate the R square out of sample\n",
    "def calculate_r2_out_of_sample(y_true, y_pred):\n",
    "    ss_res = np.sum((y_true - y_pred) * (y_true - y_pred))\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true)) * (y_true - np.mean(y_true)))\n",
    "    r2_out_of_sample = 1 - (ss_res / ss_tot)\n",
    "    return r2_out_of_sample\n",
    "\n",
    "best_r2 = -np.inf\n",
    "best_num_layers = None\n",
    "best_model = None\n",
    "\n",
    "for num_layers, study in studies.items():\n",
    "    best_params = study.best_params\n",
    "    neurons_per_layer = best_params['neurons_per_layer']\n",
    "    activation = best_params['activation']\n",
    "    optimizer = best_params['optimizer']\n",
    "    learning_rate = best_params['learning_rate']\n",
    "    batch_size = best_params['batch_size']\n",
    "\n",
    "    final_model, _ = create_model(study.best_trial, num_layers)\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "    final_model.fit(X_train, y_train, epochs=5, batch_size=batch_size, validation_split=0.2, callbacks=[es], verbose=1)\n",
    "\n",
    "    y_pred = final_model.predict(X_test)\n",
    "    test_loss, test_mae = final_model.evaluate(X_test, y_test)\n",
    "    r2_out_of_sample = calculate_r2_out_of_sample(y_test, y_pred)\n",
    "    print(f'Test MAE for {num_layers} layers: {test_mae}')\n",
    "    print(f'R² out of sample for {num_layers} layers: {r2_out_of_sample}')\n",
    "    \n",
    "        # Check if this model is the best one\n",
    "    if r2_out_of_sample > best_r2:\n",
    "        best_r2 = r2_out_of_sample\n",
    "        best_num_layers = num_layers\n",
    "        best_model = final_model\n",
    "    \n",
    "print(f'Best R² out of sample is {best_r2} for the model with {best_num_layers} layers')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-16T14:42:48.364810100Z",
     "start_time": "2024-05-16T14:42:48.351682800Z"
    }
   },
   "id": "39b7053c70a71b1c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Write the analysis here."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ac1d3e5e7065a8f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Question3.\tPick the model that performs the best out of sample, and interpret its output by doing the following analysis of variable importance:\n",
    " \n",
    "#### a.\tFirst, for all stock characteristics, get variable importance by setting one predictor at a time to zero and finding the decrease in out of sample R2. Show a table of the 10 most important variables according to this measure, and give an economic interpretation. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ec9178d01cdd48b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 基线样本外 R^2\n",
    "y_pred_baseline = best_model.predict(X_test)\n",
    "r2_baseline = calculate_r2_out_of_sample(y_test, y_pred_baseline)\n",
    "\n",
    "# 逐个变量置零并计算样本外 R^2 下降\n",
    "variable_importance = {}\n",
    "for i in range(X_test.shape[1]):\n",
    "    X_test_zeroed = X_test.copy()\n",
    "    X_test_zeroed[:, i] = 0\n",
    "    y_pred_zeroed = best_model.predict(X_test_zeroed)\n",
    "    r2_zeroed = calculate_r2_out_of_sample(y_test, y_pred_zeroed)\n",
    "    r2_decrease = r2_baseline - r2_zeroed\n",
    "    variable_importance[i] = r2_decrease\n",
    "\n",
    "# 找到最重要的10个变量\n",
    "important_variables = sorted(variable_importance.items(), key=lambda item: item[1], reverse=True)[:10]\n",
    "important_variables_df = pd.DataFrame(important_variables, columns=['Variable Index', 'Decrease in R^2'])\n",
    "\n",
    "print(important_variables_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T12:50:46.987698Z",
     "start_time": "2024-05-17T12:50:46.975482Z"
    }
   },
   "id": "fc6a49d66d139ce2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### b.\tSecond, get a measure of the joint importance of all our “macro predictors” (i.e., those taken from Welch and Goyal 2008), by setting them all to zero and finding the decrease in out of sample R2. Comment on how important macroeconomic variables are relative to stock characteristics in predicting returns."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "40185452cb535d8c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 假设宏观预测变量的索引是已知的\n",
    "macro_predictors_indices = macro.columns  # 用实际索引替换\n",
    "\n",
    "X_test_macro_zeroed = X_test.copy()\n",
    "X_test_macro_zeroed[:, macro_predictors_indices] = 0\n",
    "y_pred_macro_zeroed = best_model.predict(X_test_macro_zeroed)\n",
    "r2_macro_zeroed = calculate_r2_out_of_sample(y_test, y_pred_macro_zeroed)\n",
    "r2_macro_decrease = r2_baseline - r2_macro_zeroed\n",
    "\n",
    "print(f'Decrease in R^2 when macro predictors are set to zero: {r2_macro_decrease}')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1984453adaecf8da"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### c.\tRepeat the two steps above, but by using a measure of the sensitivity of predictions to each input variable, as outlined in the lectures."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f267bd2ccc5a337c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 计算每个变量的敏感性\n",
    "sensitivity = {}\n",
    "epsilon = 1e-5  # 微小扰动\n",
    "\n",
    "for i in range(X_test.shape[1]):\n",
    "    X_test_perturbed = X_test.copy()\n",
    "    X_test_perturbed[:, i] += epsilon\n",
    "    y_pred_perturbed = best_model.predict(X_test_perturbed)\n",
    "    sensitivity[i] = np.mean(np.abs(y_pred_perturbed - y_pred_baseline))\n",
    "\n",
    "# 找到最敏感的10个变量\n",
    "sensitive_variables = sorted(sensitivity.items(), key=lambda item: item[1], reverse=True)[:10]\n",
    "sensitive_variables_df = pd.DataFrame(sensitive_variables, columns=['Variable Index', 'Sensitivity'])\n",
    "\n",
    "print(sensitive_variables_df)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6bca202bc82e2f87"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Write the analysis here."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69a805ba0e51a224"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Question4.\tFit a penalised linear model (LASSO) to the same data, using validation data to pick the best penalty (e.g., you can use the “sklearn” package in Python to do this easily). Compare its test data performance to the neural network. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49fef10cff6c665b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Fit LASSO model with cross-validation\n",
    "lasso = LassoCV(cv=5, random_state=random_state).fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_lasso = lasso.predict(X_test)\n",
    "\n",
    "# Calculate MSE and R^2\n",
    "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
    "r2_lasso = r2_score(y_test, y_pred_lasso)\n",
    "\n",
    "print(f'LASSO MSE: {mse_lasso}')\n",
    "print(f'LASSO R^2: {r2_lasso}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-16T14:42:49.697373800Z",
     "start_time": "2024-05-16T14:42:49.684375300Z"
    }
   },
   "id": "6ad3a77dc3734b9a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Write the analysis here."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "494f1307303b57a8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Question5.\tSuppose somebody tells you to collect 10 more micro or macro variables that can predict returns and are not in our current dataset. How would you choose those variables, based on the intuitions you have gained in this project?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "798e7a758c84badd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-16T15:56:13.553463100Z",
     "start_time": "2024-05-16T15:56:13.524932700Z"
    }
   },
   "id": "827f740a5f139314"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Write the analysis here."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "709d2c66bed22f34"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1d23953189b3768c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
